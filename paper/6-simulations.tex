\section{Simulations}
\label{simulations}

\subsection{Overview}

We demonstrate the utility of these approaches using simulations. Each simulation is defined by a data-generating process with a known effect function, which allows us to compute true test set errors. Each run of each simulation generates a dataset, which we split into training and test samples. We perform $F$-fold cross validation on the training data and compute each model selection metric on each validation fold. I.e. for each training fold, we estimate $M$ different treatment effect functions $\hat\tau_m$ and for each of those we calculate each metric using the data in the corresponding validation fold. The metrics are averaged for each model across all folds. Each model selection approach then selects the model among the $M$ models that minimizes (or maximizes, when appropriate) its corresponding metric. The models selected by each approach on the basis of cross-validation are refit on the full training data and applied to the test set. The test-set treatment effect estimates of each model are compared to the known effects to calculate the true cost of using each approach for model selection. Each simulation is repeated multiple times. All of the code used to set up, run, and analyze the simulations is feely available on \href{https://github.com/som-shahlab/ITE-model-selection}{github}.

\subsubsection{Data-generating processes, sampling, and data-splitting}

We use the sixteen simulations from \citet{Powers:2017wd}, each of which we repeat $50$ times. In each repetition, $1000$ samples are used for training and $2000$ are used for testing. We use $2-$, $5-$, and $10-$fold cross validation.

\subsubsection{Models}

We fit several treatment effect models to each simulated dataset. We limit ourselves to two-model conditional mean regressions: given a model specification, we fit two separate models to predict the outcomes of the individuals who were treated and not treated and take the difference in predicted outcomes as the estimated individual treatment effect. This is so that we can calculate outcome prediction MSE as a selection metric for comparison. As a result we do not use any models that directly estimate the individual treatment effect (e.g. causal forests). The models we use are gradient boosted trees (number of trees ranging from $1$ to $500$, tree depth of $3$, shrinkage of $0.2$ and minimum $3$ individuals per node) and elastic nets ($\alpha=0.5$, $\lambda \in [e^{-5}, e^2]$). These models give us a range of high-performing linear and nonlinear models to select among. For efficiency, we only consider combinations of $\hat\mu_1$ and $\hat\mu_0$ that were fit using the same method with the same hyperparameters. This constraint need not be enforced in practice (i.e. $\hat\mu_0$ could be fit using a linear model and $\hat\mu_1$ fit using a random forest). All models are fit using the caret R package.

\subsubsection{Model selection approaches}

The following metrics are used to select among models in each simulation:

% This table is terrible. Need to 1) get Type to be multicolumn 2) fix vertical spacing b/t lines 3) make Ref section much thinner

\begin{center}
\begin{tabu}{|m{6cm}|c|c|}
	\hline
	 \rowfont[c]{\bfseries} Metric & Reference section & Type \\
	 \hline
	 Random & NA &  NA \\
	 \hline
	 Covariate matched-pairs ITE MSE & \ref{match-mse} & ITE loss \\
	 Transformed outcome ITE MSE & \ref{match-mse} & ITE loss \\
	 Covariate matched-pairs ITE decision cost &  \ref{sec:gain-value} & ITE loss (Value) \\
	 Transformed outcome ITE decision cost (neg. generalized gain) &  \ref{sec:gain}, \ref{sec:gain-value} & ITE loss (Value) \\
	 \hline
	 Traditional gain &  \ref{sec:gain} & Value \\ 
	 Decision value &  \ref{sec:value} & Value \\
	 \hline
	 C-for-benefit &  \ref{other} & AUC \\
	 Qini coefficient (gain-at-k AUC) &  \ref{other} & AUC \\
	 Value-at-k AUC &  \ref{other} & AUC \\
	 \hline
	 Est. treatment effect strata ITE MSE &  \ref{te-match} & Other \\
	 \hline
	 Outcome prediction MSE &  \ref{sec:pred-error} & Outcome loss \\
	 \hline
\end{tabu}
\end{center}

Taking the model that minimizes (or maximizes, when appropriate) one of these metrics on average across validation folds defines a model selection approach. 

We split the approaches into several categories based on the theoretical motivation for each. The ITE loss approaches all minimize metrics of the form of equation \ref{te-error}. The ``value'' approaches are those that have some theoretical equivalence to maximizing the decision value of a model. The AUC-style approaches are briefly described in section \ref{other}. 

Note that the ``random'' metric assigns a random number to each model, which means that the model selected by that metric is chosen uniformly at random from the available models. 

In simulations with biased treatment assignment, we estimate a propensity score with logistic regression. That estimated propensity is used in place of the true value in calculating all metrics that require a propensity score.

\subsubsection{Evaluation metrics}

Let the model $\hat\tau_m$ selected by optimizing metric $h$ in cross-validation be written as $\hat\tau^{*_h}$. 

We are interested in the quantities

\[
\tau MSE_h = E[ (\hat\tau^{*_h} (X) - \tau(X))^2 ]
\]

and 

\[
v_h = E[ Y| W =\hat d^{*_h} (X)]
\]

which we unbiasedly estimate in a large test set $\mathcal{T}$ via

\begin{equation}
\label{true-mse}
\tau MSE^{(\mathcal{T})}_h = \frac{1}{|\mathcal{T}|}\sum_{i \in \mathcal{T}} (\hat\tau^{*_h} (x_i) - \tau(x_i))^2
\end{equation}

and 

\begin{equation}
\label{true-value}
v^{(\mathcal{T})}_h = \frac{1}{|\mathcal{T}|}\sum_{i \in \mathcal{T}} \mu_{\hat d^{*_h}(x_i)}(x_i)
\end{equation}

Where $\hat d^{*_h}(x_i) = I(\hat\tau^{*_h}(x_i) > 0)$ as before. 

$\tau MSE_{h}^{(\mathcal{T})}$ calculates how well the selected model estimates the treatment effect for individuals in the test set. $C^{(\mathcal{T})}$ is the decision value of applying the treatment policy $\hat d(x)$ derived from each selected model $\hat \tau (x)$ to the individuals in the test set.

These are both useful metrics, although only the first ($\tau MSE$, sometimes called ``precision in estimating heteorgenous effects'', or PEHE) has typically been used in simulation studies. To see why the test set decision value is also important, consider two models ($A$ and $B$) that estimate the same treatment effect for all individuals except for two ($i=1$ and $i=2$). Let $\tau(x_1) = \tau(x_2) = 0.1$ (i.e. both individuals would benefit from the treatment). Model $A$ estimates $\hat\tau_A(x_1) = -0.1$ and $\hat\tau_A(x_2) = 0.1$. In other words, it incorrectly suggests not treating individual $1$, although the absolute difference $|\hat\tau_A(x_1)-\tau(x_1)| = 0.2$ is quite small, so model $A$ is not heavily penalized by $\tau MSE$. Model $B$ estimates $\hat\tau_B(x_1) = 0.1$ and $\hat\tau_B(x_2) = 100$. Model $B$ correctly suggests the treatment for both individuals, but the absolute difference $|\hat\tau_B(x_2)-\tau(x_2)| = 99.9$ is large and is heavily penalized by $\tau MSE$. What we want is a model that correctly assigns treatment to the individuals who stand to benefit from it. Using $\tau MSE$ in this case would favor model $A$ even though it leads to the mistreatment of more individuals than model $B$ does. However, $\tau MSE$ is still a useful metric. There may be cases where a researcher is interested in the precise magnitude of the effect for each individual, perhaps so that she can allocate scarce resources most effectively.

We also calculate their minima over all the computed models $\hat\tau_m \in \{\hat\tau_1 \dots \hat\tau_M\}$:

\[
\tau MSE^{(\mathcal{T})}_{*} = \underset{m}{\text{min}} \ \ \frac{1}{|\mathcal{T}|}\sum_{i \in \mathcal{T}} (\hat\tau_m (x_i) - \tau(x_i))^2
\]

\[
v^{(\mathcal{T})}_{*} = \underset{m}{\text{max}} \ \ \frac{1}{|\mathcal{T}|}\sum_{i \in \mathcal{T}} \mu_{\hat d_m(x_i)}(x_i)
\]

These quantities represent the best possible performance that any model selection method could acheive given the available models. We also calculate these using the true model $\hat\tau = \tau$, which represents the best achievable performance without specifying any models a-priori:

\[
\tau MSE^{(\mathcal{T})}_{**} = 0
\]

\[
v^{(\mathcal{T})}_{**} = \frac{1}{|\mathcal{T}|}\sum_{i \in \mathcal{T}} \mu_{ d(x_i)}(x_i)
\]

We use these minima to calculate the improvements obtained by each model selection approach relative to optimal baselines, which allows for comparison between different simulations. The relative values of $\tau MSE^{(\mathcal{T})}_{h}$ and $C^{(\mathcal{T})}_{h}$ within one test set and one set of models are

\[
	R\text{-} \tau MSE^{(\mathcal{T})}_{h} = 
	\frac{\tau MSE^{(\mathcal{T})}_{h} - \tau MSE^{(\mathcal{T})}_{**}}{\tau MSE^{(\mathcal{T})}_{*} - \tau MSE^{(\mathcal{T})}_{**}}
\]

\[
	R\text{-}  v^{(\mathcal{T})}_{h} = 
	\frac{ v^{(\mathcal{T})}_{h} -  v^{(\mathcal{T})}_{**}}{ v^{(\mathcal{T})}_{*} -  v^{(\mathcal{T})}_{**}}
\]

The closer to zero these are, the better the performance of the model selection method.

\subsection{Results}

Changing the number of cross-validation folds seems to have little effect, while moving from unbiased to biased treatment assignment tends to negatively impact the performance of the selected model regardless of the selection approach. 

Random model selection leads to reasonable performance in many simulations because all of the available models perform only somewhat worse than the best among them. This is reflective of real-world use where poor models or hyperparameter combinations are removed a-priori. 

ITE loss-minimizing approaches perform well, but those that employ covariate matching are notably worse off in several simulations, likely due to the difficulty of finding good covariate matches in high dimensions. All of the approaches related to value estimation perform similarly, as expected, since the metrics they are optimizing have equivalent differences in expectation (theorem \ref{gain-value}). In some simulations with biased assignment, the traditional formulation of gain does poorly where our generalized version succeeds, presumably because our formulation correctly incorporates the propensity score. Of the AUC-style approaches, the Qini coefficient performs poorly, C-for-benefit performs reasonably, and our proposed value-at-k AUC does well. Stratifying on the predicted treatment effect as described in section \ref{te-match} fares poorly, as predicted. A straightforward application of outcome prediction MSE does very well.



