\begin{abstract}

Practitioners in medicine, business, political science, and other fields are increasingly aware that decisions should be personalized to each patient, customer, or voter. A given treatment (e.g. a drug or advertisement) should be administered only to those who will respond most positively, and certainly not to those who will be harmed by it. Individual-level treatment effects (ITEs) can be estimated with tools adapted from machine learning, but different models can yield contradictory estimates. Unlike risk prediction models, however, treatment effect models cannot be easily evaluated against each other using a held-out test set because the true treatment effect itself is never directly observed. Besides outcome prediction accuracy, several methods that use held-out data to evaluate treatment effects models have been proposed, but they are largely unknown or cloistered within disciplines. We present a review of these methods, demonstrate theoretical relationships,  generalize them into a unifying framework, and suggest variations. All methods effectively calculate a loss between the ITE estimated from the training set and an ITE unbiasedly estimated from a validation set. We demonstrate the behavior of these methods using simulations of both randomized and observational data. Researchers estimating individual treatment effects should use the methods examined here instead of outcome prediction error to select between models.

\end{abstract} 