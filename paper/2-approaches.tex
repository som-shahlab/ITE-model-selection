\section{Metrics for treatment effect model selection via data-splitting}
\label{approaches}

As we have established, we are interested in statistics that, when optimized over a set of available model predictions on a validation set, also tend to minimize the $\tau$-risk $E[l(\hat\tau, \tau)]$. For the purposes of our discussion, we will focus on risk under squared error loss. Here we describe three approaches that fit the bill.

The first approach is to minimize the $\mu$-risk $E[l(\hat\mu_{W}(X), Y)]$. As we have seen, this quantity is easy to estimate. Furthermore, a perfect model of the potential outcomes implies a perfect model of the treatment effect, so we are justified in optimizing this quantity.

An alternative is to maximize the value of the treatment policy $\hat d(x) = I(\hat\tau(x) > 0)$ which indicates which individuals we expect to benefit from the treatment. The \emph{value} of a decision policy $\hat d$ is $v = E[Y|W=\hat d(X)]$. In other words, the value is the expected outcome of an individual when all individuals are treated according to the policy $\hat d$. If larger values of the outcome are more desirable (e.g. lifespan, click-through rate, approval ratings), then the policy that maximizes the value is optimal, and vice-versa. As we will see, this quantity is also simple to estimate in a few ways. If all we are interested is the treatment decision (and not the treatment effect itself), then we are already justified in maximizing value. However, $\hat\tau = \tau$ is a maximizer of the decision value $v$, so it may be justifiable to use the decision value to optimize for treatment effect estimation.

The last approach is to directly estimate the $\tau$-risk. We will see that there are several methods for doing so.

\subsection{$\mu$-risk}
\label{sec:pred-error}

There are many simple examples where minimizing the mean-squared error of predicted outcomes badly fails to select the model with the most accurate treatment effect \cite{Rolling:2013kz}. Despite this, we can attempt to use prediction error to select among treatment effect models. Assuming the treatment effect model is built by regressing the outcomes onto the covariates and treatment to obtain $\hat\mu_0$ and $\hat\mu_1$ (e.g.. with an S-learner or T-learner), we can estimate $\mu$-risk with

\begin{equation}
	\widehat{\mu\text{-risk}} = \frac{1}{|\mathcal{V}|} \sum_{i \in \mathcal{V}}^{|\mathcal{V}|}  
	(\hat \mu_{w_i} (x_i) - y_i)^2
\label{murisk}
\end{equation}
 
For individuals in the validation set who were treated ($w=1$), we estimate their outcome using the treated model and assess error, and vice-versa for the untreated. This is equivalent to estimating the predictive risk separately for $\hat\mu_1$ and $\hat\mu_0$. 

If treatment assignment is nonrandom, we can appropriately weight each residual with the estimated inverse propensity of observing an individual $x_i$ with treatment $w_i$

\begin{equation}
	\widehat{\mu\text{-risk}}_{IPTW} = \frac{1}{|\mathcal{V}|} \sum_{i \in \mathcal{V}}^{|\mathcal{V}|}  
	\frac{(\hat \mu_{w_i} (x_i) - y_i)^2}{\check p_{w_i}(x_i)}
\label{murisk-iptw}
\end{equation}

Where $p_{w_i}(x_i) = P(W=w_i | X=x_i)$. This is the propensity score if $w_i = 1$ and one minus the propensity score if $w_i = 0$. The notation $\check a$ indicates a quantity that is estimated using only data in the validation set, whereas $\hat a$ is estimated using only data in the training set. The effect of this is to create the correct ``pseduo-population'' that would have been observed under random assignment. I.e. if a treated individual $x_i$ had a probability of $0.1$ of being assigned the treatment under the observed nonrandom assignment, their residual should be weighted by a factor of $10$ to account for the 9 other individuals who would have received that treatment had the assignment been random.

\subsection{Value}
\label{sec:value}

\citet{Kapelner:3baXYEjR} and \citet{Zhao:2017wa} propose the same validation set estimator for the value of a treatment effect model:

\begin{equation}
\label{value}
\hat v_{IPTW} = \frac{1}{|\mathcal{V}|}\sum_{\mathcal{V}} \frac{y_i I(w_i=\hat d(x_i))}{\check p_{w_i}(x_i)}
\end{equation}

where again $\check p_{w_i}(x_i)$ is an estimate of $P(W=w_i | X=x_i)$.  We call this the inverse propensity of treatment weighted (IPTW) value estimator.\footnote{
We should note that this estimator is closely related to one commonly used in the direct marketing literature called uplift:

\begin{equation}
\label{gain-basic}
	\frac{1}{|\mathcal V |} \left(
		  \frac{\sum_{\mathcal{V}} y_i  \hat d(x_i) w_i}{\sum_{\mathcal{V}}  \hat d(x_i) w_i} - 
		  \frac{\sum_{\mathcal{V}} y_i  \hat d(x_i) (1-w_i)}{\sum_{\mathcal{V}}  \hat d(x_i)  (1-w_i)} 
		  \right)
		  \sum_{\mathcal{V}} \hat d(x_i) 
\end{equation}

This is actually a special case of

\begin{equation}
\label{gain}
	\hat g  = \dfrac{1}{|\mathcal V |} \left[ \sum_{\mathcal{V}} \dfrac{y_i  \hat d(x_i) w_i}{p(x_i)} - \sum_{\mathcal{V}} \dfrac{y_i  \hat d(x_i) (1-w_i)}{1-p(x_i)} \right]
\end{equation}

To see this, we rewrite equation \ref{gain-basic} as:
\[
	 \dfrac{1}{|\mathcal V |} \underbrace{ \frac{ \sum_{\mathcal{V}} \hat d(x_i)}{\sum_{\mathcal{V}}  \hat d(x_i) w_i} }_{1/\hat p}
		  	\sum_{\mathcal{V}} y_i  \hat d(x_i) w_i - 
		\dfrac{1}{|\mathcal V |}  \underbrace{ \frac{ \sum_{\mathcal{V}} \hat d(x_i)}{\sum_{\mathcal{V}}  \hat d(x_i)  (1-w_i)} }_{1/ (1-\hat p)}
		  	\sum_{\mathcal{V}} y_i  \hat d(x_i) (1-w_i) 
\]

The multipliers underbraced above are unbiased estimates of $1/ p_{w_i}$ because of the conditional independence of $\hat d(X)$ and $W$. Thus the traditional estimator (equation \ref{gain-basic}) is suitable for use when the propensity score $p(x) = p$ is a constant, as is the case in randomized experiments.

To see the relationship between uplift and value, note that
\[
\begin{array}{rcl}	
	E[\hat g | X=x] & = & E[\tau(x) \hat d(x) | X=x]  \\
	& =  & (\mu_1(x)-\mu_0(x))  \hat d(x)  \\
	E[\hat v | X=x] & = & E[Y | W=\hat d(x), X=x]  \\
	& = & \mu_0(x)(1-\hat d(x)) + \mu_1(x)\hat d(x)
\end{array}
\]

Consider two policies $\hat d_a$ and $\hat d_b$ and their respective estimated values $\hat v_a, \hat v_b$  and gains $\hat g_a, \hat g_b$. The expected difference in value between the two models is 

\[
\begin{array}{rcl}
E[\hat v_a - \hat v_b] 

& = & E[E[\hat v_a | X] - E[\hat v_b|X]] \\

&=& E \left[
	\mu_0(X)(1-\hat d_a(X)) + \mu_1(X)\hat d_a(X) 
      - \mu_0(X)(1-\hat d_b(X)) -  \mu_1(X)\hat d_b(X)
\right] \\

&=& E \left[
	  \hat d_a(X) (\mu_1(X)  - \mu_0(X) ) 
	- \hat d_b(X) (\mu_1(X)  - \mu_0(X) )
\right] \\

&=& E \left[
	  E[\hat g_a | X] ) 
	- E[\hat g_b | X] )
\right] \\

&=& E \left[ \hat g_a - \hat g_b \right] \\
\end{array}
\]

Thus a model optimizing any unbiased estimate of uplift also optimizes any unbiased estimate of value in expectation.
}

In the randomized setting where $p_{w_i}(x_i) = 0.5$, we can imagine that two side-by-side experiments were run, one in which treatments were assigned according to the model ($W = \hat d(X)$) and one in which they were assigned according to the opposite recommendation ($W = 1 - \hat d(X)$). The data in the validation set are a concatenation of the data from these two experiments. To estimate the value of our model, we average the outcomes of individuals in the first experiment and ignore the data from the second experiment. This is essentially what the estimator in equation \ref{value} is doing. When $p_{w_i}(x_i) \ne 0.5$, we must appropriately weigh the outcomes according to the probability of treatment to accomplish the same goal. \citet{Kapelner:3baXYEjR} give a similar explanation, but omit the role of the propensity score. \citet{Zhao:2017wa} provide a short proof that $\hat v$ is unbiased for the true value $v = E[Y|W = \hat d(X)]$. 

A problem with this estimator is that it depends on the correctness of the propensity model. In addition, it only utilizes a portion of the data: $\hat v_{IPTW}$ throws away individuals whose treatments do not match $\hat d(x_i)$. In the spirit of \citet{Dudik:tn}, \citet{Athey:wj} overcome this by using a doubly-robust formulation

\begin{equation}
	\hat v_{DR} = \frac{1}{|\mathcal{V}|}\sum_{\mathcal{V}}
	\hat d(x_i)
	\left(
	\check\mu_1(x_i) - \check\mu_0(x_i)+ (2w_i -1)\frac{y_i - \check\mu_{w_i}(x_i)}{\check p_{w_i}(x_i)}
	\right)
\label{value-dr}
\end{equation}

where $\check\mu_{w_i}(x_i)$ and $\check p(x_i)$ can be estimated with standard regression methods using data in the validation set. \citet{Athey:wj} use an estimator of this form in order to fit a policy model and establish theoretical gaurentees, whereas here we will use it to select among several pre-fit models.

\subsection{$\tau$-risk}

We have already seen that $\frac{1}{|\mathcal{V}|}\sum_{i \in \mathcal{V}}^{|\mathcal{V}|}  (\hat \tau_m (x_i)- \tau(x_i))^2$ is infeasible. A natural workaround is to replace $\tau(x_i)$ with an estimate derived from the validation set:

\[
\frac{1}{|\mathcal{V}|}\sum_{i \in \mathcal{V}}^{|\mathcal{V}|}  (\hat \tau (x_i) -  \check \tau_i)^2
\]

Here, $\check \tau$ is a plug-in estimate of $\tau$ estimated using data in the validation set $\mathcal{V}$. 

\citet{Rolling:2013kz} propose an estimator $\check \tau_i$ based on matched treated and control individuals in the validation set. Briefly, for each individual $i$ in the validation set they use Mahalanobis distance matching to identify the most similar individual $\bar{i}$ in the validation set with the opposite treatment ($w_i \ne w_{\bar i}$) and compute $\check \tau_i = (2w_i -1)(y_i - y_{\bar i})$ as the plug-in estimate of $\tau(x_i)$. 

\begin{equation}
\widehat{\tau\text{-risk}}_{match} = \frac{1}{|\mathcal{V}|}\sum_{i \in \mathcal{V}}^{|\mathcal{V}|}  (\hat \tau (x_i) - (2w_i -1)(y_i - y_{\bar i}))^2
\label{trisk-match}
\end{equation}

They prove under general assumptions and a squared-error loss that a more mathematically tractable version of their algorithm has selection consistency, meaning that it correctly selects the best model as the number of individuals goes to infinity. They conjecture that the practical version of the algorithm retains this property.

A downside of this approach are that Mahalanobis matching scales relatively poorly and matches become difficult to find in high-dimensional covariate spaces. An alternative proposed by \citet{Gutierrez:2016tq} takes advantage of the fact that the IPTW-weighted (transformed) outcome $\frac{(2W-1)Y}{p_W(X)}$ is an estimator for $\tau$:

\begin{equation}
\widehat{\tau\text{-risk}}_{IPTW} = 
	\frac{1}{|\mathcal{V}|}\sum_{i \in \mathcal{V}}^{|\mathcal{V}|}  
	\left(\hat \tau (x_i) - \frac{(2w_i -1)y_i}{\check p_{w_i}(x_i)}\right)^2
\label{trisk-iptw}
\end{equation}

This formulation is also used for model fitting in the transformed-outcome forest of \citet{Powers:2017wd} and in some versions of the causal tree in \citet{Athey2015}.

Our final approach deviates from this schema and is due to \citet{Nie:2017vi}, who propose minimizing

\begin{equation}
\widehat{\tau\text{-risk}}_{R} = 
	\frac{1}{|\mathcal{V}|}\sum_{i \in \mathcal{V}}^{|\mathcal{V}|}  
	((y_i - \check m(x_i)) - (w_i - \check p(x_i))\hat\tau (x_i))^2
\label{trisk-r}
\end{equation}

The function $\check m(x)$ is an estimate of $E[Y|X]$ which can be obtained by regressing $Y$ onto $X$ without using the treatment $W$. \citet{Nie:2017vi} provide theoretical and empirical results that show how fitting models using an objective of this form (with some additional stipulations) can outperform T- and S-learning. We propose using this same construction to select among models fit by arbitrary means.

