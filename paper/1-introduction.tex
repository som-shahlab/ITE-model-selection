\section{Introduction}

For a particular individual $i$ characterized by a vector of covariates $x_i$, a decision maker must chose between prescribing an intervention $w_i=1$ or no intervention $w_i=0$. The intervention (treatment) may be a drug, an advertisement, a campaign email etc. The decision-maker's goal is to maximize some outcome $y_i$ for that patient or customer, which may be their lifespan, their net purchases, their political engagement etc. Inferring the effects of the treatment on the outcome is a causal inference problem. Using the potential outcomes framework \cite{Rubin2005}, we write the conditional means of the potential outcomes as $\mu_0(x) = E[Y|W=0,X=x]$ and $\mu_1(x) = E[Y|W=1,X=x]$. The estimand in question is the conditional average treatment effect $\tau(x_i) = \mu_1(x_i) - \mu_0(x_i)$, which is the expected difference in potential outcomes under the alternative interventions for the individual in question. In different fields this quantity is alternatively called the individual treatment effect, individual causal effect, individual benefit, or individual lift. If the true conditional mean functions $\mu_w(x)$ are known, the rule (policy) that assigns each individual $x_i$ their optimal treatment is $d(x) = \underset{w \in \{0,1\}}{\text{argmax}} \ \ \mu_w(x)$, or, alternatively, $d(x) = I(\tau(x) > 0)$ where $I$ is the indicator function. Generally, the conditional mean functions $\mu_w$ are unknown, meaning that there is uncertainty about the optimal treatment policy $d(x)$.

Prior to the development of modern statistical methods, policies were generally one-size-fits-all prescriptions based on estimates of the average treatment effect $\bar\tau = E[\tau(X)]$ \cite{Segal:ub}. These experiments limit individual heterogeneity by imposing strict criteria on the population under study \cite{?_INCLUSION_CRITERIA}. In contrast, multiple fields today are attempting to leverage modern statistical technology and real-world data to tailor decisions to individuals; this phenomena is exemplified by the rise of personalized medicine \cite{Ferreira:2017fv} and targeted advertisement \cite{Ascarza:2018ie, ?_POLITICAL_TARGETING}. Decision-makers recognize that treating to the average, while expedient, does not result in the best outcome for all individuals \cite{Kravitz:2004fa,Segal:ub}. 

Note that estimating individual treatment effects is not the same as estimating personalized risks or prognoses with prediction models (e.g. for a heart attack, customer churn, or non-voting). Prediction models only predict what would happen to the individual given standard practice, not the difference of what would happen if a treatment were or were not given. As such, prediction models by themselves are often of little practical utility unless the effects of available treatments are known and relatively constant. If that is not the case, targeting high-risk individuals is not an optimal strategy: there may be high-risk individuals who do not respond or respond negatively to the treatment, and low-risk individuals who would respond very positively \cite{Ascarza:2018ie}.

There are currently a suite of methods to estimate individual treatment effects from randomized data. The process of estimating these effects is alternatively referred to as heterogenous effect modeling, uplift modeling, or individual treatment effect modeling. These approaches can also be used for observational data if certain assumptions are met or if combined with propensity score or matching techniques.

A manual subgroup analysis is the traditional approach to heterogenous treatment effect estimation. A subgroup analysis partitions the population of individuals into manually-specified subgroups and typically estimates an average treatment effect in each subgroup using traditional methods (i.e. linear or logistic regression) \cite{Gail:1985ft}. This approach requires a high degree of domain knowledge is prone to multiple-hypothesis testing problems if subgroups are not pre-specified. 

An alternative is to use any supervised learning method (e.g. LASSO, random forest, neural network) to fit functions $\hat\mu_0$ and $\hat\mu_1$ that estimate the conditional means $\mu_0$ and $\mu_1$ of the potential outcomes. These estimates are then used to estimate the treatment effect $\hat\tau(x) = \hat\mu_1(x) - \hat\mu_0(x)$ \cite{Gutierrez:2016tq, Austin:2012cy, Snowdn:2011ef}. This can be done by regressing the observed outcomes on the covariates in the untreated group to get $\hat\mu_0$ and regressing the observed outcomes on the covariates in the treated group to get $\hat\mu_1$. This method is alternately referred to as g-computation, simulated twins, counterfactual regression, or conditional mean regression. Similarly, it is possible to fit a single model $\hat\mu(x,w)$ and estimate the treatment effect as $\hat\tau(x) = \hat\mu(x,1) - \hat\mu(x,0)$, although it appears that the two-model approach has theoretical advantages \cite{Alaa:tj}.

Modeling the conditional means is a valid approach, but many have noted that since the object of interest is the treatment effect we may be better off modeling it directly without appeal to the correctness of $\hat\mu_0$ and $\hat\mu_1$. Approaches in this vein include \citet{Zhao:2017vi}, \citet{Athey:2016wm}, and \citet{Powers:2017wd}. 

Among the variety of approaches and the number of hyper-parameter settings within each approach, which is best? As is the case with all statistical learning problems, there is no free lunch \cite{Wolpert:1996fp}; different methods will give better or worse estimates depending on the application and there cannot be a universally-best method. Indeed, using a large set of diverse simulations, \citet{Dorie:2017uo} find that the only somewhat consistent predictor of the success of a causal inference method is its ability to ``flexibly'' model the conditional means or treatment effect. Although that result surely depends upon the particulars of their simulations, it parallels the common knowledge in the machine learning literature that deep nets and additive regression trees often outperform linear models for real-world applications. However, even limiting ourselves to flexible treatment effect modeling methods, we are left with a panoply of approaches and hyper-parameter settings to chose from. 

We digress briefly to discuss the standard supervised learning setting where the task is to estimate $y$ given $x$ by building an estimator $\hat{f}(x)$. There we can use the diversity of machine learning approaches to our advantage by performing model selection. Given $M$ modeling approaches and/or hyper-parameter settings, we build $M$ estimators $\{\hat f_1, \hat f_2 \dots \hat f_m \dots \hat f_M\}$. The quantity of interest in this case is the expected error of the model when it is applied to new data, according to some loss $l$. We express that as $e = E[l(\hat f_m(X), Y)]$. The idea of model selection is to estimate this expected error for each of our models and find the model that minimizes it. There are several ways to estimate this error, including information criteria and Vapnik-Chervonenkis dimension, but data-splitting is the easiest and most widely applicable \cite{esl:2009wc}. Before fitting the models, the observations are randomly split into training and validation samples. The models are fit on the training sample $\mathcal{T}$ and evaluated on the validation sample $\mathcal{V}$. The error of each model is estimated with $\hat e_m = \frac{1}{|\mathcal{V}|}\sum_{i \in \mathcal{V}}^{|\mathcal{V}|} l(\hat f_m (x_i), y_i)$. In cross-validation, this process is repeated round-robin across different random splits of the data and the estimated errors are averaged per model.

This approach breaks down for treatment effect estimation because the true treatment effect is never observed in any sample. We would like to evaluate models $\{\hat\tau_1, \hat\tau_2 \dots \hat \tau_m \dots \hat \tau_M\}$ by estimating their error on a validation set via $\hat e_m = \frac{1}{|\mathcal{V}|}\sum_{i \in \mathcal{V}}^{|\mathcal{V}|}  l(\hat \tau_m (x_i), \tau(x_i))$ (this quantity has been called the precision in estimating heterogenous effects or PEHE \cite{Hill2011}). The problem is that we never observe $\tau(x_i)$ directly (we only see one of the two potential outcomes) and thus have nothing to compare $\hat\tau(x_i)$ to. This estimator of the treatment effect error is thus infeasible.

Several treatment effect model selection approaches have been suggested in the literature, but none of them enjoys the wide use and dominance that prediction error cross-validation has in the supervised learning setting. Most approaches are not general-purpose in that they require that the set of estimators $\{\hat\tau_1, \hat\tau_2 \dots \hat \tau_m \dots \hat \tau_M\}$ come from a specific class of models. For example, \citet{Powers:2017wd} and \citet{Athey2015} both use selection methods that are specific to the models they propose. The Focused Information Criterion (FIC) \cite{Claeskens:2003ck} is a promising approach, but as of yet cannot be used to select between most machine learning estimators \cite{Jullum:2012uo}. \citet{Alaa:tj} propose an empirical Bayes approach for optimal prior selection. \citet{Nie:2017vi} circumvent the problem with a different method of treatment effect estimation which precludes comparison to standard approaches, but allows for internal model selection using prediction error. 

It is clear that we lack a go-to general-purpose approach for applied researchers to select among treatment effect models. The absence of a standard leaves the door open for poor practice, which we hope to combat by presenting our findings here.

In section \ref{approaches}, we describe many proposed general-purpose approaches for treatment effect model selection.

We have already seen that $\hat e_m = \frac{1}{|\mathcal{V}|}\sum_{i \in \mathcal{V}}^{|\mathcal{V}|}  l(\hat \tau_m (x_i), \tau(x_i))$ is infeasible. A natural approach is to replace $\tau(x_i)$ with an estimate derived from the validation set:

\begin{equation}
\check e = \frac{1}{|\mathcal{V}|}\sum_{i \in \mathcal{V}}^{|\mathcal{V}|}  l(\hat \tau (x_i), \check \tau_i)
\label{te-error}
\end{equation}

Where $\check \tau$ is a plug-in estimate of $\tau$ estimated using data in the validation set $\mathcal{V}$. 

Our primary finding in section \ref{theory} is that, although not directly apparent in some cases, all of these approaches can be generalized to this procedure (or related to it). We also show that $\check e$ is consistent for $e$ with certain combinations of $\check\tau$ and $l$. In section \ref{variants}, we leverage our understanding to create treatment effect model selection procedures for specific cases;  we also conjecture the inefficacy of alternative procedures. Section \ref{other} explores approaches that are inspired by area-under-the-curve statistics. The simulations described in section \ref{simulations} underscore our findings.
