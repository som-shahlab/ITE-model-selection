\section{Theory}
\label{theory}

\subsection{Connection value and $\tau$-error}
\label{sec:gain-value}

It is straightforward to see how the matching MSE and transformed outcome MSE are special cases of $\tau$-error using squared-error loss and two different estimators for $\check\tau$. Here we show that optimizing decision value also optimizes a particular $\tau$-error in expectation. To do so, we first introduce \emph{gain}, which has been previously used in the direct marketing literature to select treatment effect models. We will show that gain can be seen as a $\tau$-error and that optimizing gain is equivalent in expectation to optimizing value. 

Gain (sometimes also called \emph{lift}) is defined as follows:

\begin{equation}
\label{gain-basic}
	\hat \gamma = \frac{1}{|\mathcal V |} \left(
		  \frac{\sum_{\mathcal{V}} y_i  \hat d(x_i) w_i}{\sum_{\mathcal{V}}  \hat d(x_i) w_i} - 
		  \frac{\sum_{\mathcal{V}} y_i  \hat d(x_i) (1-w_i)}{\sum_{\mathcal{V}}  \hat d(x_i)  (1-w_i)} 
		  \right)
		  \sum_{\mathcal{V}} \hat d(x_i) 
\end{equation}

The first term inside the parentheses is the average outcome among \emph{treated} individuals in the validation set who were also recommended for treatment by the model. The second term is the average outcome among \emph{untreated} individuals in the validation set who were recommended for treatment by the model. The resulting difference is an estimate of the average treatment effect among individuals recommended for treatment by the model. That number is multiplied by the total number of individuals in the validation set recommended for treatment by the model. 

This estimator (which we call traditional gain) is actually a special case of the generalized gain:

\begin{equation}
\label{gain}
\begin{array}{rcl}
	\hat g & =& \dfrac{1}{|\mathcal V |} \sum_{\mathcal{V}} \dfrac{y_i  \hat d(x_i) w_i}{p(x_i)} - \sum_{\mathcal{V}} \dfrac{y_i  \hat d(x_i) (1-w_i)}{1-p(x_i)} \\
	&=& \dfrac{1}{|\mathcal V |} \sum_{\mathcal{V}} y^{\dagger}_i \hat d(x_i)
\end{array}
\end{equation}

To see this, we rewrite equation \ref{gain-basic} as:
\[
	 \dfrac{1}{|\mathcal V |} \underbrace{ \frac{ \sum_{\mathcal{V}} \hat d(x_i)}{\sum_{\mathcal{V}}  \hat d(x_i) w_i} }_{1/\hat p}
		  	\sum_{\mathcal{V}} y_i  \hat d(x_i) w_i - 
		\dfrac{1}{|\mathcal V |}  \underbrace{ \frac{ \sum_{\mathcal{V}} \hat d(x_i)}{\sum_{\mathcal{V}}  \hat d(x_i)  (1-w_i)} }_{1/ (1-\hat p)}
		  	\sum_{\mathcal{V}} y_i  \hat d(x_i) (1-w_i) 
\]

The multipliers underbraced above are unbiased estimates of $1/\hat p_{w_i}$ because of the conditional independence of $\hat d(X)$ and $W$. 

Thus the traditional formula for gain is a version of our formula that is suitable for use when $p(x) = p$ is a constant, as is the case in randomized experiments. By and large, the direct marketing literature has focused on randomized data - to our knowledge this is the first time a gain estimator has been constructed for observational data. 

Note that gain is a special case of equation \ref{te-error} where $l(\hat \tau (x_i), \check \tau_i) = -\check \tau_i  I(\hat \tau (x_i) > 0)$ (we call this the decision cost loss) and $\check\tau_i = y_i^{\dagger}$ (the transformed outcome). Gain can thus been seen as a $\tau$-error. In our experiments we test both this $\tau$-error and  a $\tau$-error with the same loss but with the matching estimator from section \ref{match-mse} instead of the transformed outcome.

\begin{theorem}
\label{gain-value}
In expectation, a model that maximizes estimated gain also maximizes estimated value.
\end{theorem}

\begin{proof}
Note that
\[
\begin{array}{rcl}	
	E[\hat g | X=x] & = & E[Y^{\dagger} \hat d(x) | X=x]  \\
	& =  & (\mu_1(x)-\mu_0(x))  \hat d(x)  \\
	E[\hat v | X=x] & = & E[Y | W=\hat d(x), X=x]  \\
	& = & \mu_0(x)(1-\hat d(x)) + \mu_1(x)\hat d(x)
\end{array}
\]

Consider two policies $\hat d_a$ and $\hat d_b$ and their respective estimated values $\hat v_a, \hat v_b$  and gains $\hat g_a, \hat g_b$. The expected difference in value between the two models is 

\[
\begin{array}{rcl}
E[\hat v_a - \hat v_b] 

& = & E[E[\hat v_a | X] - E[\hat v_b|X]] \\

&=& E \left[
	\mu_0(X)(1-\hat d_a(X)) + \mu_1(X)\hat d_a(X) 
      - \mu_0(X)(1-\hat d_b(X)) -  \mu_1(X)\hat d_b(X)
\right] \\

&=& E \left[
	  \hat d_a(X) (\mu_1(X)  - \mu_0(X) ) 
	- \hat d_b(X) (\mu_1(X)  - \mu_0(X) )
\right] \\

&=& E \left[
	  E[\hat g_a | X] ) 
	- E[\hat g_b | X] )
\right] \\

&=& E \left[ \hat g_a - \hat g_b \right] \\

\end{array}
\]

If the estimated value of $\hat d_a$ is larger than than of $\hat d_b$, we expect the relationship between their gains to be the same. Since this relation holds between any two policies $\hat d_a$ and $\hat d_b$, the policy that maximizes estimated gain also maximizes estimated value in expectation.

\end{proof}

To our knowledge, this is the first result that demonstrates the link between gain and decision-theoretic value and demonstrates why maximizing gain works in practice. Since gain is a $\tau$-error, we have demonstrated a connection between decision value and $\tau$-error. 

\subsection{Properties of some $\tau$-errors}

\subsubsection{Consistency under squared-error loss}

Here we show for the first time that any unbiased and consistent estimate $\check\tau$ of the treatment effect calculated on the validation set can be used for estimation of the generalization error under squared-error loss. 

\begin{theorem}
If $\check\tau$ is unbiased for $\tau$, then 
\[
E\left[\frac{1}{|\mathcal{V}|}\sum_{i \in \mathcal{V}}^{|\mathcal{V}|}  (\hat \tau(X_i) - \check \tau_i)^2\right] = E[(\hat\tau(X) - \tau(X))^2] + E[(\check\tau(X) - \tau(X))^2]
\]
\end{theorem}

\begin{proof}
\begin{equation}
	E\left[ \frac{1}{|\mathcal{V}|}\sum_{i \in \mathcal{V}}^{|\mathcal{V}|}  (\hat \tau(X_i) - \check \tau_i)^2 \right]  
	= 
	E[ (\hat \tau(X) - \tau + \tau  - \check \tau)^2 ] \\
\label{expected-plug-in-mse}
\end{equation}

The quantity in the sum can be expressed as
\[
E[ (\hat \tau(X) - \tau)^2] + 2E[(\hat \tau(X) - \tau)(\tau - \check\tau)] + E[(\tau - \check\tau)^2]
\]

Using the law of total expectation, we rewrite the second term as $E[E[(\hat \tau(x_i) - \tau(x_i)(\tau(x_i) - \check\tau_i)|X=x_i]]$. Since $\check\tau_i$ and $\hat\tau(x_i)$ are independent, this factors to $E[E[\hat \tau(x_i) - \tau(x_i)|X=x_i]E[\tau(x_i) - \check\tau_i|X=x_i]]$, which is $0$ because $E[\tau(x_i) - \check\tau_i|X=x_i] = 0$ by unbiasedness. Thus equation \ref{expected-plug-in-mse} becomes

\[
	E\left[ \frac{1}{|\mathcal{V}|}\sum_{i \in \mathcal{V}}^{|\mathcal{V}|}  (\hat \tau(x_i) - \check \tau_i)^2 \right]  
	=
	E[ (\hat \tau(X) - \tau(X))^2] + E[(\tau(X) - \check\tau(X))^2]
\]

\end{proof}

The expected estimated error when we use $\check\tau$ as an estimate for $\tau$ is the expected error of our model $\hat\tau$ plus the expected error of our plug-in estimate $\check\tau$. Consequently, the estimated error we obtain is likely to be greater than the true generalization error of our model. However, our estimate can still be used to select between treatment effect models since the surplus error $E[(\tau(X) - \check\tau(X))^2]$ does not depend on the model $\hat \tau(X)$. This result implies that using consistent unbiased estimators $\check\tau$ and squared-error loss leads to consistent model selection. If $\check\tau$ is consistent, the surplus error asymptotically approaches its expected value, which does not vary with $\hat\tau$. In the limit, when subtracting $\tau$-errors from two models $\hat\tau_A$ and $\hat\tau_B$, the surplus errors cancel exactly to yield the desired $E[ (\hat \tau_A(X) - \tau(X))^2] - E[ (\hat \tau_B(X) - \tau(X))^2]$.

\begin{comment}
    \begin{theorem}
    If $\check\tau_i$ is unbiased and consistent for $\tau_i$, then $\underset{m}{\emph{argmin}}\frac{1}{|\mathcal{V}|}\sum_{i \in \mathcal{V}}^{|\mathcal{V}|}  (\hat \tau_m(x_i) - \check \tau_i)^2$ is a consistent estimator of $\underset{m}{\emph{argmin}} \ E[(\hat\tau_m(X) - \tau(X))^2]$.
    \end{theorem}
    
    \begin{proof}
    
    ($^*$waves hands$^*$) The value of the estimated error for a model $\hat\tau_m$ is $\check e_m = \frac{1}{|\mathcal{V}|}\sum_{i \in \mathcal{V}}^{|\mathcal{V}|}  (\hat \tau_m(x_i) - \check \tau_i)^2 $. As $n$ goes to infinity, this should converge in probability to its expected value, which is a constant $e_m + d$, where $e_m$ is the true generalization error $E[(\tau(X) - \hat\tau(X))^2]$ and $d=E[(\tau(X) - \check\tau(X))^2]$ is a constant that doesn't depend on the model. We compare two models $\hat\tau_A$ and $\hat\tau_B$ on the basis of their estimated error $\check e_A$ and $\check e_B$. In the limit, $\check e_A > \check e_B \iff  e_A + d >  e_B + d \iff  e_A >  e_B$ ($^*$waves hands$^*$) 
    
    \end{proof}
\end{comment}

\subsubsection{Unbiasedness under multiplicative loss}

\begin{theorem}
For losses of the form $l(\check\tau(x), \hat\tau(x)) = \check\tau(x) f(\hat\tau(x))$, if $E[\check\tau(X)|X] = \tau(x)$, then $E[\check e] = E [  l(\tau, \hat\tau) ]$.
\end{theorem}

\begin{proof}

\[
\begin{array}{rcl}
	E[\check e] & = & E \left[ \dfrac{1}{| \mathcal V |} \sum_{i \in \mathcal V} \check\tau(X) f(\hat\tau(X)) \right] \\
	& = & E \left[  E[ \check\tau(X) f(\hat\tau(X)) | X] \right] \\
	& = & E \left[  E[ \check\tau(X)|X] f(\hat\tau(X))] \right] \\
	& = & E \left[  \tau(X) f(\hat\tau(X))] \right] \\
	& = & E [  l(\tau, \hat\tau) ]
\end{array}
\]

\end{proof}

We can therefore generalize gain, using any unbiased estimator of $\tau$ instead of $y^{\dagger}$. By an argument similar to that of theorem \ref{gain-value}, maximizing any form of generalized gain will maximize value in expectation.

\subsubsection{Implications}

If squared-error loss is used, only consistency in estimating a difference in generalization errors can be guaranteed, even with an unbiased estimator $\check\tau$. In other words, model selection with these approaches is not guaranteed to work (on average) unless the number of individuals in the validation set is large.

Our results do guarantee that the estimate of generalization error will be unbiased (even in finite samples) if $\check\tau$ is unbiased for $\tau$ and a multiplicative loss is used. The result should not be surprising given the relationship between these kinds of $\tau$-errors and value (section \ref{sec:gain-value}) and knowing that we can unbiasedly estimate value in finite samples. 

