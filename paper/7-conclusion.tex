\section{Conclusion}
\label{conclusion}

Although prediction error cross-validation is widely used to select between predictive models, there is no consensus on how to perform model selection for individual treatment effects models. We use simulations to examine the performance of several proposed approaches and our own adaptations of methods that have been previously used for model fitting, but not model selection. 

Our results show that $\widehat{\tau\text{-risk}}_{R}$ is the validation set metric that, when optimized, most consistently leads to the selection of a high-performing model. This conclusion strengthens the claims of \citet{Nie:2017vi} and extends the utility of their framework to a model selection setting where treatment effects models may be fit by any algorithm. 

Figure \ref{details16} shows that all of the estimators of $\tau$-risk are biased upwards (i.e. they overestimate the risk). An explanation of that phenomenon for $\widehat{\tau\text{-risk}}_{IPTW}$ can be found in \citet{Gutierrez:2016tq}. Their argument extends to $\widehat{\tau\text{-risk}}_{match}$ if the matching treatment effect estimate $(2w_i -1)(y_i - y_{\bar i})$ is unbiased. Regardless, for the purposes of model selection, the relevant quantity is the difference in estimates for different models and so this bias is of less importance. Figure \ref{details16} shows good correlation between $\tau$-risk estimated on the validation set according to various methods and the true test-set $\tau$-risk for each model. 

Interestingly, our results also show that model selection on the basis of $\hat v_{IPTW}$ or $\hat v_{DR}$ is generally suboptimal, even when the goal is to optimize $v^{(\mathcal S)}$. This is likely because these metrics are in a sense ``coarser'' than the other metrics we consider, effectively turning a regression problem into a classification problem. However, figure \ref{details16} does show that $\hat v_{IPTW}$ and $\hat v_{DR}$ have low empirical bias for $v^{(\mathcal S)}$, as they should when standard assumptions of unconfoundedness are met. Thus, unlike estimators of $\tau$-risk, it is appropriate to use these estimators to gauge the value of the final treatment decision policy created by thresholding a treatment effect model.

The primary limitation of our work is that it relies on simulations. This is a necessity because $\tau\text{-risk}_{h}^{(\mathcal{S})}$ and $v^{\mathcal S}$ cannot be calculated from real data. While no set of simulations can mimic the full range of real-world data-generating processes, our experiments span a range of linear and nonlinear potential outcome functions, high- and low-noise settings, and levels of bias in treatment assignment. 

To conclude, we advocate for the use of $\widehat{\tau\text{-risk}}_{R}$ as a model selection metric for individual treatment effects models. Researchers estimating heterogenous treatment effects need not limit themselves to a single model-fitting algorithm. There are many cases in our simulations where models fit via an R-learner are outperformed by models fit via a T-learner, or where using the elastic net confers an advantage over using gradient boosted trees, and vice-versa. Instead of relying on a single method, multiple models fit by a diverse set of algorithms should be evaluated against each other using $\widehat{\tau\text{-risk}}_{R}$ as estimated on a validation set. The best performing model should be used for estimating the individual treatment effect for future individuals.

% - other methods? qini and c-stat?

