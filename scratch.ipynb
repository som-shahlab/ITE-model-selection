{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-05T00:06:02.966894Z",
     "start_time": "2018-04-05T00:05:57.841Z"
    }
   },
   "outputs": [],
   "source": [
    "library(plyr)\n",
    "library(magrittr)\n",
    "library(distr)\n",
    "library(distrEx)\n",
    "library(gbm)\n",
    "library(Matching)\n",
    "library(caret)\n",
    "library(tidyverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-05T00:06:03.019784Z",
     "start_time": "2018-04-05T00:05:58.226Z"
    }
   },
   "outputs": [],
   "source": [
    "DGP = list()\n",
    "n = 5\n",
    "\n",
    "X1 = Norm(-1)\n",
    "X2 = Norm(1)\n",
    "DGP$X = list(X1, X2)\n",
    "\n",
    "DGP$f_W_x = function(x, w) {\n",
    "    logit_p = x[1] + x[2]\n",
    "    p = exp(logit_p) / (1 + exp(logit_p))\n",
    "    Binom(prob=p)\n",
    "}\n",
    "\n",
    "DGP$f_Y_xw = function(x, w) {\n",
    "    if(w) {\n",
    "        Weibull(scale=abs(x[1] + x[2]) + 0.7, \n",
    "                shape=1.2)\n",
    "    } else {\n",
    "        Weibull(scale= abs(x[1]) + abs(x[2]), \n",
    "                shape=1.5)\n",
    "    }\n",
    "}\n",
    "\n",
    "DGP$f_C_xw = function(x, w) {\n",
    "    Weibull(scale=4, \n",
    "            shape=1.4)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-05T00:06:03.092421Z",
     "start_time": "2018-04-05T00:05:58.980Z"
    }
   },
   "outputs": [],
   "source": [
    "#' @import dplyr\n",
    "#' @import purrr\n",
    "#' @import tidyr\n",
    "#' @import magrittr\n",
    "#' @import stringr\n",
    "#' @import caret\n",
    "#' @import Matching\n",
    "#' @import distr\n",
    "#' @import distrEx\n",
    "\n",
    "## Each patient will now get their own control Y0 and treatment Y1 distributions from \n",
    "# package distr, with parameters f(X) for more or less arbitrary f()\n",
    "# Then we'll calculate tau as E[Y1] - E[Y0] using the E() function\n",
    "# from distrEx. \n",
    "# yi = w_i*r(Y1) + (1-w_i)*r(Y0)\n",
    "# \n",
    "\n",
    "data_list_to_df = function(data_list) {\n",
    "    data_list$covariates %<>% data.frame %>% set_names(paste(\"covariate\", 1:ncol(.), sep=\"_\"))\n",
    "    data_list %$% cbind(subject, event, time, treatment, covariates) %>% data.frame\n",
    "}\n",
    "\n",
    "data_df_to_list = function(data_df) {\n",
    "    data_list = list()\n",
    "    data_list$subject = data_df %>% pull(subject)\n",
    "    data_list$outcome = data_df %>% pull(outcome) # an indicator: 1 for death, 0 for censoring\n",
    "    data_list$time = data_df %>% pull(time) # the time of either death or censoring\n",
    "    data_list$treatment = data_df %>% pull(treatment)\n",
    "    data_list$covariates = as.matrix(data_df %>% select(starts_with()))\n",
    "    return(data_list)\n",
    "}\n",
    "\n",
    "#' Generate simulated observational data\n",
    "#'\n",
    "#' Generates N tuples from the joint distribution of P(X,W,Y).\n",
    "#' Returns a list with two elements: a dataframe called data and a dataframe \n",
    "#' called aux_data containing true expecations conditional on X.\n",
    "#' @param X a list of distribution objects that generate a covariate vector\n",
    "#' @param f_W_x a function of x that returns the conditional distribution W|X=x\n",
    "#' @param f_Y_xw a function of x, w that returns the conditional distribution Y|X=x,W=w\n",
    "#' @param f_C_xw a function of x, w that returns the conditional distribution C|X=x,W=w\n",
    "#' @keywords\n",
    "#' @export\n",
    "#' @examples\n",
    "dgp = function() {\n",
    "    list(X=X, f_W_x=f_W_x, f_Y_xw=f_Y_xw, f_C_xw=f_C_xw)\n",
    "}\n",
    "# These functions e.g. f_W_X will likely be generated on-the-fly:\n",
    "# make_f_W_X_dist = function(f1, f2, ...) {\n",
    "#       function(x) SomeDist(param1 = f1(x), param2 = f2(x)... )\n",
    "# }\n",
    "\n",
    "sample1 = function(dist) {\n",
    "    r(dist)(1)\n",
    "}\n",
    "\n",
    "#' Generate simulated observational data\n",
    "#'\n",
    "#' Generates N tuples from the joint distribution of P(X,W,Y).\n",
    "#' Returns a list with two elements: a dataframe called data and a dataframe \n",
    "#' called aux_data containing true expecations conditional on X.\n",
    "#' @param DGP a data generating process, which is the list output of the dgp() function\n",
    "#' @param n the number of desired samples\n",
    "#' @keywords\n",
    "#' @export\n",
    "#' @examples\n",
    "create_data = function(DGP, n=1) {\n",
    "    x = n %>% rerun(DGP$X %>% map_dbl(sample1)) \n",
    "\n",
    "    W = x %>% map(DGP$f_W_x) \n",
    "    w = W %>% map_dbl(sample1)\n",
    "    pw = W %>% map_dbl(E) # propensity scores\n",
    "\n",
    "    Y1 = x %>% map(~DGP$f_Y_xw(.,1))\n",
    "    Y0 = x %>% map(~DGP$f_Y_xw(.,0)) \n",
    "    mu1 = Y1 %>% map_dbl(E)\n",
    "    mu0 = Y0 %>% map_dbl(E)\n",
    "    tau = mu1 - mu0\n",
    "    y = list(w,Y1,Y0) %>% \n",
    "        pmap_dbl(function(w, Y1, Y0) ifelse(w, sample1(Y1), sample1(Y0)))\n",
    "\n",
    "    C1 = x %>% map(~DGP$f_C_xw(.,0))\n",
    "    C0 = x %>% map(~DGP$f_C_xw(.,1))\n",
    "    ce = list(w,C1,C0) %>% \n",
    "        pmap_dbl(function(w, C1, C0) ifelse(w, sample1(C1), sample1(C0)))\n",
    "\n",
    "    t = pmin(y,ce)\n",
    "    d = y < ce\n",
    "\n",
    "    pc = list(w, t, C1, C0) %>% # censoring probability at t\n",
    "         pmap_dbl(function(w,t,C1,C0) ifelse(w, 1-p(C1)(t), 1-p(C0)(t)))\n",
    "\n",
    "    data = list()\n",
    "    data$subject = 1:length(w)\n",
    "    data$time = t\n",
    "    data$event = d\n",
    "    data$treatment = as.logical(w)\n",
    "    data$covariates = x %>% reduce(rbind)\n",
    "    rownames(data$covariates) = NULL\n",
    "\n",
    "    aux_data = data.frame(\n",
    "        subject=data$subject, \n",
    "        treated_mean=mu1, \n",
    "        control_mean=mu0, \n",
    "        effect=tau,\n",
    "        iptw=1/(1-w + 2*w*pw - pw),\n",
    "        ipcw=1/pc)\n",
    "\n",
    "    return(list(data=(data %>% data_list_to_df), aux_data=aux_data))\n",
    "}\n",
    "\n",
    "f1 = function(x) rep(0, nrow(x))\n",
    "\n",
    "f2 = function(x) 6 * (x[, 1] > 1) - 1 - \n",
    "    (6*pnorm(-1) -1) # take out the expectation\n",
    "\n",
    "f3 = function(x) 5 * x[, 1]\n",
    "\n",
    "f4 = function(x) {\n",
    "    1 * x[,2] * x[,4] * x[,6] + \n",
    "    2 * x[,2] * x[,4] * (1-x[,6]) +\n",
    "    3 * x[,2] * (1-x[,4]) * x[,6] + \n",
    "    4 * x[,2] * (1-x[,4]) * (1-x[,6]) +\n",
    "    5 * (1-x[,2]) * x[,4] * x[,6] + \n",
    "    6 * (1-x[,2]) * x[,4] * (1-x[,6]) +\n",
    "    7 * (1-x[,2]) * (1-x[,4]) * x[,6] + \n",
    "    8 * (1-x[,2]) * (1-x[,4]) * (1-x[,6]) - \n",
    "    5 -\n",
    "    (-0.5) # take out the expectation\n",
    "}\n",
    "\n",
    "f5 = function(x) x[, 1] + x[, 3] + x[, 5] + x[, 7] + x[, 8] + x[, 9] -\n",
    "    (0.5) # take out the expectation\n",
    "\n",
    "f6 = function(x) {\n",
    "    4 * (x[,1]>1) * (x[,3]>0) + 4 * (x[,5]>1) * (x[,7]>0) + 2 * x[,8] * x[,9] - 1 -\n",
    "    (4*pnorm(-1)-1) # take out the expectation\n",
    "}\n",
    "\n",
    "f7 = function(x) {\n",
    "  ( x[, 1]^2 + \n",
    "    x[, 2] + \n",
    "    x[, 3]^2 + \n",
    "    x[, 4] + \n",
    "    x[, 5]^2 + \n",
    "    x[, 6] + \n",
    "    x[, 7]^2 +\n",
    "    x[, 8] + \n",
    "    x[, 9]^2 ) / \n",
    "    sqrt(2) - 5 -\n",
    "    (7/sqrt(2) - 5) # take out the expectation\n",
    "}\n",
    "\n",
    "f8 = function(x) (f4(x) + f5(x)) / sqrt(2) \n",
    "    # ((-0.5 - )sqrt(2)) # take out the expectation\n",
    "\n",
    "f9 = function(x) (x[,1])^2 - 1\n",
    "\n",
    "#' Simulations \n",
    "#'\n",
    "#' @export\n",
    "schuler_DGPs = function() {\n",
    "    X = list(Norm(-1), Norm(1))\n",
    "\n",
    "    f_W_x = function(x, w) {\n",
    "        logit_p = x[1] + x[2]\n",
    "        p = exp(logit_p) / (1 + exp(logit_p))\n",
    "        Binom(prob=p)\n",
    "    }\n",
    "\n",
    "    f_Y_xw = function(x, w) {\n",
    "        if(w) {\n",
    "            Weibull(scale=abs(x[1] + x[2]) + 0.7, \n",
    "                    shape=1.2)\n",
    "        } else {\n",
    "            Weibull(scale= abs(x[1]) + abs(x[2]), \n",
    "                    shape=1.5)\n",
    "        }\n",
    "    }\n",
    "\n",
    "    f_C_xw = function(x, w) {\n",
    "        Weibull(scale=4, \n",
    "                shape=1.4)\n",
    "    }\n",
    "\n",
    "    list(\"biased\" = dgp(X, function(x,w) 0.5, f_Y_xw, f_C_xw), \n",
    "         \"unbiased\" = dgp(X, f_W_x, f_Y_xw, f_C_xw))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-05T00:06:03.197035Z",
     "start_time": "2018-04-05T00:06:03.163Z"
    }
   },
   "outputs": [],
   "source": [
    "#' @import dplyr\n",
    "#' @import purrr\n",
    "#' @import tidyr\n",
    "#' @import magrittr\n",
    "#' @import caret\n",
    "#' @import Matching\n",
    "\n",
    "# setup_fitting_1_model = function(method, tune_grid=NULL, train_index) {\n",
    "# \tfunction(data) train(\n",
    "# \t\t  x = data %>% dplyr::select(treatment, starts_with(\"covariate\")) %>% as.matrix,\n",
    "# \t\t  y = data$outcome,\n",
    "# \t\t  method = method,\n",
    "# \t\t  trControl = trainControl(method='cv', \n",
    "#                                  number=length(train_index),\n",
    "# \t\t  \t\t\t\t\t\t index=train_index,\n",
    "#                                  returnResamp=\"all\",\n",
    "#                                  savePredictions=\"all\"),\n",
    "# \t\t  tuneGrid = tune_grid)\n",
    "# }\n",
    "\n",
    "# test_estimate_hte_1_model = function(data, method, tune_grid, train_index) {\n",
    "# \tfitting_function = setup_fitting_1_model(method, tune_grid, train_index)\n",
    "# \tcf_data = data %>% # must be in this order for the train index to work\n",
    "#     \tmutate(treatment = !treatment)\n",
    "# \tfull_data = bind_rows(data, cf_data)\n",
    "# \tmodels = full_data %>% fitting_function\n",
    "# \tmodels$pred %>% # this carries with it columns with all the values of the hyperparameters\n",
    "# \t\tmutate(method = method) %>%\n",
    "# \t\t# unite(model, -pred, -obs, -rowIndex, -Resample, sep=\"^\") %>%\n",
    "# \t\tunite_(\"model\", c(\"method\", names(tune_grid)), sep=\"~\") %>%\n",
    "# \t\t# unite(model, !!!syms(c(\"method\", names(tune_grid))), sep=\"~\") %>%\n",
    "# \t    mutate(cf = ifelse(rowIndex > nrow(data), \"counterfactual\", \"factual\")) %>%\n",
    "# \t    mutate(subject = ifelse(cf==\"factual\", rowIndex, rowIndex-nrow(data))) %>%\n",
    "# \t    select(-rowIndex, -obs) %>%\n",
    "# \t    spread(cf, pred) %>% \n",
    "# \t    arrange(Resample, subject) %>%\n",
    "# \t    filter(!is.na(factual)) %>%\n",
    "# \t    inner_join(data %>% select(subject, treatment, outcome), by=\"subject\") %>%\n",
    "# \t    mutate(treated=ifelse(treatment, factual, counterfactual),\n",
    "# \t           control=ifelse(treatment, counterfactual, factual),\n",
    "# \t           effect=treated-control)\n",
    "# }\n",
    "\n",
    "\n",
    "gbm_ph_fit_predict = function(x_train, y_train, x_val, rowIndex, interaction.depth, n.minobsinnode, shrinkage, n.trees) {\n",
    "    model = gbm.fit(x_train, y_train, distribution=\"coxph\", verbose=F,\n",
    "                    n.trees=max(n.trees), interaction.depth=interaction.depth, \n",
    "                    shrinkage=shrinkage, n.minobsinnode=n.minobsinnode)\n",
    "    predict(model, x_val, n.trees=n.trees) %>% \n",
    "        data.frame %>%\n",
    "        mutate(rowIndex=rowIndex, interaction.depth=interaction.depth, \n",
    "               shrinkage=shrinkage, n.minobsinnode=n.minobsinnode) %>%\n",
    "        gather(n.trees, pred, -rowIndex, -interaction.depth, -shrinkage, -n.minobsinnode) %>%\n",
    "        mutate(n.trees = str_replace(n.trees,\"X\",\"\") %>% as.numeric)\n",
    "}\n",
    "\n",
    "# the thing that is estimated by coxph models is the log relative (to the basline) risk \n",
    "fit_model = function(data, train_index, method, tune_grid) {\n",
    "    x_train = data[train_index,] %>% dplyr::select(starts_with(\"covariate\")) %>% as.matrix\n",
    "    y_train = data[train_index,] %$% Surv(time, event)\n",
    "    x_val = data[-train_index,] %>% dplyr::select(starts_with(\"covariate\")) %>% as.matrix\n",
    "    rowIndex = data[-train_index,] %>% pull(subject)\n",
    "    \n",
    "    if(method==\"gbm\") {\n",
    "        grouped_tune_grid = tune_grid %>% \n",
    "            group_by(interaction.depth, n.minobsinnode, shrinkage) %>%\n",
    "            summarize(n.trees=list(n.trees))\n",
    "        grouped_tune_grid %>%\n",
    "            pmap(function(interaction.depth, n.minobsinnode, shrinkage, n.trees) {\n",
    "                gbm_ph_fit_predict(x_train, y_train, x_val, rowIndex,\n",
    "                                   interaction.depth, n.minobsinnode, \n",
    "                                   shrinkage, n.trees)}) %>%\n",
    "            bind_rows()\n",
    "    }\n",
    "}\n",
    "\n",
    "prep_fold_data = function(training_data, test_data) {\n",
    "\tdata = bind_rows(training_data, test_data) %>%\n",
    "\t\tmutate(rowIndex=row_number())\n",
    "\tindex = list(c(data %>% \n",
    "\t\t\t\t\tfilter(sample_type==\"training\") %>% \n",
    "\t\t\t\t\tpull(rowIndex)))\n",
    "\treturn(list(data=data, index=index))\n",
    "}\n",
    "\n",
    "test_estimate_hte = function(data, method, tune_grid, fold, fold_name) {\n",
    "\ttraining_data = data[fold,] %>% mutate(sample_type=\"training\")\n",
    "\ttest_data = data[-fold,] %>% mutate(sample_type=\"test\")\n",
    "\n",
    "\tfold_data = training_data %>% \n",
    "\t\tsplit(.$treatment) %>%\n",
    "\t\tmap(~prep_fold_data(., test_data)) #now have a list (treat => (data, fold))\n",
    "\tpredictions = fold_data %>%\n",
    "\t\tmap(~fit_model(.$data, .$index, method, tune_grid)$pred) # returns the big matrix with all test set predictions for each treatment\n",
    "\ttest_estimates = fold_data %>%\n",
    "\t    map(~select(.$data, subject, treatment, time, event, rowIndex)) %>%\n",
    "\t    list(predictions) %>%\n",
    "\t    pmap(function(data, predictions) inner_join(data, predictions, by=\"rowIndex\")) %>%\n",
    "\t    imap(function(df,name) rename_(df, .dots=setNames(\"pred\", str_c(\"est_rel_risk\", name, sep=\"_\")))) %>% # df_TRUE$pred and df_FALSE$pred become (est_rel_risk_TRUE, ..._FALSE) in the same df\n",
    "\t    reduce(inner_join, by=c(\"subject\", \"treatment\", \"time\", \"event\", names(tune_grid))) %>% # will join on all columns... if didn't want to join on params would also have to join across methods!\n",
    "\t    mutate(method=method) %>%\n",
    "\t    unite_(\"model\", c(\"method\", names(tune_grid)), sep=\"~\") %>% \n",
    "\t    mutate(fold=fold_name) %>%\n",
    "\t    mutate(est_effect=est_rel_risk_TRUE-est_rel_risk_FALSE, \n",
    "\t    \t   est_rel_risk=treatment*(est_rel_risk_TRUE) + (1-treatment)*est_rel_risk_FALSE) %>% # selects the estimated time, event from the appropriate model\n",
    "\t    select(subject, model, treatment, time, event, est_effect, est_rel_risk, fold) \n",
    "}\n",
    "\n",
    "cross_estimate_hte = function(data, method, tune_grid, train_index) {\n",
    "\ttrain_index %>%\n",
    "\timap(function(fold, fold_name) test_estimate_hte(data, method, tune_grid, fold, fold_name)) %>%\n",
    "\tbind_rows()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-05T00:06:03.722788Z",
     "start_time": "2018-04-05T00:06:03.675Z"
    }
   },
   "outputs": [],
   "source": [
    "#' @import dplyr\n",
    "#' @import purrr\n",
    "#' @import tidyr\n",
    "#' @import magrittr\n",
    "#' @import caret\n",
    "#' @import Matching\n",
    "\n",
    "# group data into all test folds\n",
    "# for each test fold, do the internal matching\n",
    "# record the matched patient\n",
    "find_matches = function(data) {\n",
    "\ttreated_match = Match(Tr=data$treatment, \n",
    "\t   \t\t\t\t\t  X=data.matrix(data%>%select(starts_with(\"covariate\"))),\n",
    "\t    \t\t\t\t  replace=T, estimand=\"ATT\") \n",
    "\tcontrol_match = Match(Tr=!data$treatment, # (separate so one treated doesn't get two controls ambiguously)\n",
    "\t    \t\t\t\t  X=data.matrix(data%>%select(starts_with(\"covariate\"))),\n",
    "\t    \t\t\t\t  replace=T, estimand=\"ATT\")\n",
    "\tdata.frame(subject = data$subject[c(treated_match$index.treated, control_match$index.treated)], # all subjects\n",
    "\t\t\t   match = data$subject[c(treated_match$index.control, control_match$index.control)]) # their matches\n",
    "}\n",
    "\n",
    "create_cv_index = function(data, n_folds=5) {\n",
    "\t\tcreateFolds(data$treatment, k=n_folds) %>% # hold-out indices\n",
    "    \tmap(~(data$subject)[-.]) #  complement of each index, in terms of the subject IDs\n",
    "}\n",
    "\n",
    "\n",
    "# see:\n",
    "# Fewell, Z., Hernán, M. A., Wolfe, F., Tilling, K., Stata, H. C., 2004. (n.d.). Controlling for time-dependent confounding using marginal structural models. Pdfs.Semanticscholar.org\n",
    "# Rodriguez, G. (2014). Survival Models. In Lecture Notes on Generalized Linear Models (pp. 1–34).\n",
    "ipcw = function(data, n_time_intervals=10) {\n",
    "\tinterval_data = data %>% \n",
    "\t    mutate(event = !event) %>% # \"event\" is now censoring, not death\n",
    "\t    mutate(event_interval = ntile(time, n_time_intervals))\n",
    "\tpseudo_data = list(subject = interval_data %$% unique(subject), \n",
    "                   interval = interval_data %$% unique(event_interval)) %>%\n",
    "    cross_df %>%\n",
    "    inner_join(interval_data, by=\"subject\") %>%\n",
    "    filter(interval <= event_interval) %>% # keep only pseudo-observations before or at event time\n",
    "    mutate(event = ifelse(interval==event_interval, event, FALSE)) # before the real time of censoring, set censoring=F\n",
    "    pCt_XW = glm.fit( # no global intercept\n",
    "\t    x = pseudo_data %>% \n",
    "\t            select(subject, interval, treatment, starts_with(\"covariate\")) %>% \n",
    "\t            mutate(interval_copy = interval) %>%\n",
    "\t            mutate(trash=1) %>%\n",
    "\t            spread(interval, trash, fill=0) %>% # give each interval its own intercept\n",
    "\t            select(-interval_copy, -subject) %>%\n",
    "\t            data.matrix(),\n",
    "\t    y = pseudo_data %>% pull(event),\n",
    "\t    family = binomial(link=\"cloglog\")) %$% \n",
    "\t\tfitted.values\n",
    "    data.frame(pCt_XW = pCt_XW,\n",
    "          \t   subject = pseudo_data$subject) %>%\n",
    "\t    group_by(subject) %>%\n",
    "\t    summarize(cens_prob_at_event_time = prod(1-pCt_XW)) %>%\n",
    "\t    ungroup() %>%\n",
    "\t    mutate(est_ipcw = 1/cens_prob_at_event_time) %>%\n",
    "\t    inner_join(data, by=\"subject\") %>%\n",
    "\t    select(subject, est_ipcw)\n",
    "}\n",
    "\n",
    "iptw = function(data) {\n",
    "\tpW_X = glm.fit(\n",
    "\t\tx = data %>% select(starts_with(\"covariate\")) %>% as.matrix,\n",
    "\t\ty = data %>% pull(\"treatment\"),\n",
    " \t\tfamily = binomial(link=\"logit\")) %$% \n",
    "\tfitted.values\n",
    "\tdata.frame(pW_X = pW_X, \n",
    "\t\t\t   treatment=data$treatment,\n",
    "\t\t\t   subject=data$subject) %>%\n",
    "\t\tmutate(est_iptw = 1/(1-treatment + 2*treatment*pW_X - pW_X)) %>%\n",
    "\t\tselect(subject, est_iptw)\n",
    "}\n",
    "\n",
    "#' Prepares simulated data for experiments\n",
    "#'\n",
    "#' @param DGP a list created by a call to dgp()\n",
    "#' @param n_train number of samples to train and validate on\n",
    "#' @param n_test number of samples to test on\n",
    "#' @param n_folds the number of folds to use for treatment effect cross-validation\n",
    "#' @keywords\n",
    "#' @export\n",
    "#' @examples\n",
    "setup_data = function(DGP, n_train, n_test, n_folds) {\n",
    "\tsimulation = create_data(DGP, n_train+n_test) \n",
    "\tdata = simulation$data %>% \n",
    "\t    mutate(set = ifelse(subject > n_train, \"test\", \"training\"))\n",
    "\taux_data = simulation$aux_data %>%\n",
    "\t\tmutate(set = ifelse(subject > n_train, \"test\", \"training\"))\n",
    "\n",
    "\tcv_index = data %>% \n",
    "\t    filter(set==\"training\") %>%\n",
    "\t    create_cv_index(n_folds=n_folds)\n",
    "\ttest_index = list(\"training\" = (data %>% filter(set==\"training\") %>% pull(subject)))\n",
    "\n",
    "\tcv_held_out = cv_index %>% \n",
    "    \tmap(~test_index$training[!(test_index$training %in% .)])\n",
    "\theld_out = c(cv_held_out, list(\"training\" = (data %>% filter(set==\"test\") %>% pull(subject))))\n",
    "\tmatches = held_out %>% # need to find matches for each individual within each training fold... \n",
    "\t    map(~ data.frame(subject=.) %>%\n",
    "\t    \tinner_join(data, by=\"subject\") %>% \n",
    "\t    \tfind_matches()) %>%\n",
    "\t    bind_rows(.id=\"fold\")\n",
    "\n",
    "\taux_data = aux_data %>%\n",
    "\t\tinner_join(matches, by=\"subject\") %>%\n",
    "\t\tinner_join(data %>% select(subject, treatment), by='subject') %>%\n",
    "\t\tinner_join(iptw(data), by=\"subject\") %>% \t# I'm cheating a little bit here because I use test data to fit these\n",
    "\t\tinner_join(ipcw(data), by=\"subject\") %>% \t# but the models should be underfit anyways so that data shouldn't add much...\n",
    "\t\tselect(-treatment) # can fix this later but won't really change it\n",
    "\treturn(list(data=data, aux_data=aux_data, cv_index=cv_index, test_index=test_index))\n",
    "}\n",
    "\n",
    "#' Estimates conditonal mean regression models on the data via caret. \n",
    "#' Does cross-estimation on the training data and runs each model trained on the full training set on the test set.\n",
    "#' Returns all out-of-sample estimates from each model\n",
    "#'\n",
    "#' @param data  data \n",
    "#' @param models  models \n",
    "#' @param cv_index cv_index\n",
    "#' @param test_index  test_index \n",
    "#' @keywords\n",
    "#' @export\n",
    "#' @examples\n",
    "get_estimates = function(data, models, cv_index, test_index) {\n",
    "\ttraining_data = data %>% \n",
    "\t    filter(set==\"training\") \n",
    "\tcv_estimates = models %>% # cross validate on CV data (ignore the test set)\n",
    "\t\tmap(~cross_estimate_hte(training_data, .$method, .$tune_grid, cv_index)) %>%\n",
    "\t    bind_rows() \n",
    "\ttest_estimates = models %>% # now train on all CV data and test on the test set\n",
    "\t\tmap(~cross_estimate_hte(data, .$method, .$tune_grid, test_index)) %>% \n",
    "\t\tbind_rows() \n",
    "\treturn(list(cv_estimates=cv_estimates, test_estimates=test_estimates))\n",
    "}\n",
    "\n",
    "compute_cv_metrics = function(estimates) {\n",
    "\testimates  %>%\n",
    "\t    # dplyr::group_by(!!!syms(c(param_names, \"fold\"))) %>% # I do this for each fold\n",
    "\t    dplyr::group_by(model, fold) %>% # I do this for each fold\n",
    "\t    # mutate(est_effect_test_match = est_effect_covariate_matching(treatment, outcome, subject, match),\n",
    "\t    \t   # est_effect_test_trans = est_effect_transformed_outcome(treatment, outcome, ip_weights)) %>%\n",
    "\t    dplyr::summarize(#### framework: ###\n",
    "\t    \t\t\t\t match_mse = est_effect_covariate_matching(treatment, outcome, subject, match) %>% loss_squared_error(est_effect),\n",
    "\t    \t\t\t\t trans_mse = est_effect_transformed_outcome(treatment, outcome, true_ip_weights) %>% loss_squared_error(est_effect), \n",
    "\t\t\t\t\t\t trans_mse_est_prop = est_effect_transformed_outcome(treatment, outcome, est_ip_weights) %>% loss_squared_error(est_effect), \n",
    "\t    \t\t\t\t match_decision = est_effect_covariate_matching(treatment, outcome, subject, match) %>% loss_decision(est_effect),\n",
    "\t    \t\t\t\t trans_decision = est_effect_transformed_outcome(treatment, outcome, true_ip_weights) %>% loss_decision(est_effect), # aka gain!\n",
    "\t    \t\t\t\t trans_decision_est_prop = est_effect_transformed_outcome(treatment, outcome, est_ip_weights) %>% loss_decision(est_effect), # aka gain!\n",
    "\t    \t\t\t\t #### value: ####\n",
    "\t\t\t\t\t\t value = -value(est_effect, treatment, outcome, weights=true_ip_weights),\n",
    "\t\t\t\t\t\t gain = -gain(est_effect, treatment, outcome),\n",
    "\t\t\t\t\t\t value_est_prop = -value(est_effect, treatment, outcome, weights=est_ip_weights),\n",
    "\t\t\t\t\t\t # #### broken: ####\n",
    "\t    \t\t\t\t prediction_error = loss_squared_error(est_outcome, outcome),\n",
    "\t                     est_te_strata = est_effect_transformed_outcome(treatment, outcome, est_effect) %>% loss_squared_error(est_effect),\n",
    "\t                     #### ranking: ####\n",
    "\t                     c_benefit = -c_benefit(est_effect, treatment, outcome),\n",
    "\t                     qini = -qini(est_effect, treatment, outcome, true_ip_weights),\n",
    "\t                     qini_est_prop = -qini(est_effect, treatment, outcome, est_ip_weights),\n",
    "\t                     value_auc = -value_auc(est_effect, treatment, outcome, true_ip_weights),\n",
    "\t                     value_auc_est_prop = -value_auc(est_effect, treatment, outcome, est_ip_weights),\n",
    "\t                     ### random: ####\n",
    "\t                     random = random_metric()\n",
    "\t                     ) %>%\n",
    "\t   \tdplyr::ungroup() %>% dplyr::select(-fold) %>% dplyr::group_by(model) %>% # Then average over the folds\n",
    "\t    dplyr::summarize_all(mean, na.rm=T)\n",
    "}\n",
    "\n",
    "compute_test_metrics = function(estimates) {\n",
    "\testimates  %>%\n",
    "\t    dplyr::group_by(model) %>% # there should only be one fold\n",
    "\t    dplyr::summarize(true_hte_error = loss_squared_error(est_effect, true_effect),\n",
    "\t                     true_value = -true_value(est_effect, true_effect, true_mean)) \n",
    "}\n",
    "\n",
    "#' Estimates estimated cv errors and true test errors (the latter via true values in aux_data). \n",
    "#'\n",
    "#' @param cv_estimates  cv_estimates \n",
    "#' @param test_estimates test_estimates \n",
    "#' @param aux_data aux_data\n",
    "#' @keywords\n",
    "#' @export\n",
    "#' @examples\n",
    "get_errors = function(cv_estimates, test_estimates, aux_data) {\n",
    "\tcv_error = cv_estimates %>% \n",
    "\t\tinner_join(aux_data, by=c(\"subject\", \"fold\")) %>%\n",
    "    \tcompute_cv_metrics() %>%\n",
    "        gather(selection_method, error, -model)\n",
    "\tmin_cv_error = cv_error %>%\n",
    "\t    group_by(selection_method) %>%\n",
    "\t    filter(error == min(error, na.rm=T)) %>%\n",
    "\t    sample_n(1) %>% # if there are ties for the lowest error, break at random\n",
    "\t    select(-error) %>% ungroup()\n",
    "\ttest_error = test_estimates %>% \n",
    "\t\tinner_join(aux_data, by=c(\"subject\", \"fold\")) %>%\n",
    "\t    compute_test_metrics()\n",
    "\toracle_error = test_error %>%\n",
    "\t\tgather(selection_method, error, -model) %>%\n",
    "\t\tgroup_by(selection_method) %>%\n",
    "\t    filter(error == min(error, na.rm=T)) %>%\n",
    "\t    sample_n(1) %>%\n",
    "\t\tselect(-error) %>% ungroup() %>%\n",
    "\t\tmutate(selection_method = str_c(\"oracle_selector\", selection_method, sep=\"_\"))\n",
    "\ttrue_selection_error = min_cv_error %>%\n",
    "\t\tbind_rows(oracle_error) %>%\n",
    "\t    inner_join(test_error, by=\"model\") %>%\n",
    "\t    bind_rows(data.frame(model=\"truth\", selection_method=\"oracle\", true_hte_error=0,  # this is the true model\n",
    "\t    \t\t\t\t\t true_value=-(aux_data %>% filter(set==\"test\") %$% true_value(true_effect, true_effect, true_mean)))) # this needs to be evaluated just over the test set!!!\n",
    "\t    # mutate(optimal_deficiency = -true_hte_value(true_effect, true_effect, true_mean)) # %>%\n",
    "\t    # mutate(scenario=scenario, n_folds=n_folds, training_percent=training_percent, rep=rep)\n",
    "\treturn(list(cv_error=cv_error, test_error=test_error, true_selection_error=true_selection_error))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-05T00:06:11.350092Z",
     "start_time": "2018-04-05T00:06:04.780Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      ": package ‘bindrcpp’ was built under R version 3.2.5"
     ]
    }
   ],
   "source": [
    "datas = setup_data(DGP, 500, 500, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-05T00:05:26.650755Z",
     "start_time": "2018-04-05T00:05:26.614Z"
    }
   },
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in xj[i]: invalid subscript type 'list'\n",
     "output_type": "error",
     "traceback": [
      "Error in xj[i]: invalid subscript type 'list'\n"
     ]
    }
   ],
   "source": [
    "models = list(\n",
    "    gbm_spec = list(method = \"gbm\",\n",
    "                tune_grid = expand.grid(n.trees = seq(1,501,20), \n",
    "                                        interaction.depth=3, \n",
    "                                        shrinkage = 0.2, \n",
    "                                        n.minobsinnode=3)))\n",
    "herp = datas %$% get_estimates(data, models, cv_index, test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-05T00:05:10.530102Z",
     "start_time": "2018-04-05T00:05:05.365Z"
    }
   },
   "outputs": [],
   "source": [
    "method = \"gbm\"\n",
    "tune_grid = expand.grid(n.trees = seq(1,501,20), \n",
    "                        interaction.depth=3, \n",
    "                        shrinkage = 0.2, \n",
    "                        n.minobsinnode=5)\n",
    "train_index = 1:500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-05T00:05:20.131675Z",
     "start_time": "2018-04-05T00:05:20.024Z"
    }
   },
   "outputs": [],
   "source": [
    "herp = fit_model(datas$data, datas$cv_index[[1]], method, tune_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-05T00:04:28.275654Z",
     "start_time": "2018-04-05T00:04:28.232Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>rowIndex</th><th scope=col>interaction.depth</th><th scope=col>shrinkage</th><th scope=col>n.minobsinnode</th><th scope=col>n.trees</th><th scope=col>pred</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>17337</th><td>995</td><td>3</td><td>0.2</td><td>5</td><td>501</td><td>3.187675</td></tr>\n",
       "\t<tr><th scope=row>17338</th><td>996</td><td>3</td><td>0.2</td><td>5</td><td>501</td><td>-0.3993727</td></tr>\n",
       "\t<tr><th scope=row>17339</th><td>997</td><td>3</td><td>0.2</td><td>5</td><td>501</td><td>-1.060509</td></tr>\n",
       "\t<tr><th scope=row>17340</th><td>998</td><td>3</td><td>0.2</td><td>5</td><td>501</td><td>4.018024</td></tr>\n",
       "\t<tr><th scope=row>17341</th><td>999</td><td>3</td><td>0.2</td><td>5</td><td>501</td><td>0.4351027</td></tr>\n",
       "\t<tr><th scope=row>17342</th><td>1000</td><td>3</td><td>0.2</td><td>5</td><td>501</td><td>1.01304</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllll}\n",
       "  & rowIndex & interaction.depth & shrinkage & n.minobsinnode & n.trees & pred\\\\\n",
       "\\hline\n",
       "\t17337 & 995 & 3 & 0.2 & 5 & 501 & 3.187675\\\\\n",
       "\t17338 & 996 & 3 & 0.2 & 5 & 501 & -0.3993727\\\\\n",
       "\t17339 & 997 & 3 & 0.2 & 5 & 501 & -1.060509\\\\\n",
       "\t17340 & 998 & 3 & 0.2 & 5 & 501 & 4.018024\\\\\n",
       "\t17341 & 999 & 3 & 0.2 & 5 & 501 & 0.4351027\\\\\n",
       "\t17342 & 1000 & 3 & 0.2 & 5 & 501 & 1.01304\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "      rowIndex interaction.depth shrinkage n.minobsinnode n.trees       pred\n",
       "17337      995                 3       0.2              5     501  3.1876747\n",
       "17338      996                 3       0.2              5     501 -0.3993727\n",
       "17339      997                 3       0.2              5     501 -1.0605088\n",
       "17340      998                 3       0.2              5     501  4.0180244\n",
       "17341      999                 3       0.2              5     501  0.4351027\n",
       "17342     1000                 3       0.2              5     501  1.0130397"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tail(derp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
